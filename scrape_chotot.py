import os
import json
import time
import requests
import gspread
import random
import re
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cáº¤U HÃŒNH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://www.chotot.com"
START_URL = "https://www.chotot.com/mua-ban-nhac-cu-ha-noi?price=0-2100000&f=p&limit=20"
SHEET_ID = "14tqKftTqlesnb0NqJZU-_f1EsWWywYqO36NiuDdmaTo"
SHEET_NAME = "Chá»£ tá»‘t"
MAX_PAGES = 12
MAX_CONSECUTIVE_EMPTY = 3
SLEEP_BETWEEN_PAGES = 4.5
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
]
def log(message):
    now = datetime.now().strftime("%H:%M:%S")
    print(f"[{now}] {message}")
def get_telegram_config():
    return {
        "token": os.environ.get("TELEGRAM_BOT_TOKEN"),
        "chat_id": os.environ.get("TELEGRAM_CHAT_ID")
    }
def setup_driver():
    log("Khá»Ÿi táº¡o Chrome headless...")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(f"user-agent={random.choice(USER_AGENTS)}")
    driver = webdriver.Chrome(options=options)
    return driver
def connect_google_sheet():
    log("Káº¿t ná»‘i Google Sheets...")
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds_json_str = os.environ.get("GOOGLE_CREDENTIALS")
    if not creds_json_str:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y GOOGLE_CREDENTIALS")
    creds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(creds_json_str), scope)
    client = gspread.authorize(creds)
    sh = client.open_by_key(SHEET_ID)
    try:
        worksheet = sh.worksheet(SHEET_NAME)
        log(f"TÃ¬m tháº¥y sheet: {SHEET_NAME}")
    except gspread.WorksheetNotFound:
        worksheet = sh.add_worksheet(title=SHEET_NAME, rows=2000, cols=10)
        headers = ["Title", "Price", "Link", "Time Posted", "Location", "Seller", "Views", "Scraped At", "Hidden"]
        worksheet.append_row(headers)
        log("Táº¡o sheet & header má»›i")
    if worksheet.col_count < 9:
        worksheet.resize(cols=10)
    headers = worksheet.row_values(1)
    if len(headers) < 9 or headers[8] != "Hidden":
        worksheet.update_cell(1, 9, "Hidden")
        log("Äáº£m báº£o cá»™t Hidden á»Ÿ I")
    return worksheet
def get_images_from_detail(link):
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    try:
        resp = requests.get(link, headers=headers, timeout=12)
        if resp.status_code != 200:
            log(f"Detail {link} status {resp.status_code}")
            return [], []
        soup = BeautifulSoup(resp.text, "html.parser")
        images = []
        videos = []
        # 1. Æ¯u tiÃªn JSON-LD (náº¿u cÃ³ máº£ng image chÃ­nh thá»©c)
        json_ld_tags = soup.find_all("script", type="application/ld+json")
        for tag in json_ld_tags:
            try:
                data = json.loads(tag.string or "{}")
                if isinstance(data, dict) and "image" in data:
                    img_list = data["image"]
                    if isinstance(img_list, str):
                        img_list = [img_list]
                    if isinstance(img_list, list):
                        for img in img_list:
                            if isinstance(img, str) and "cdn.chotot.com" in img:
                                # Giá»¯ nguyÃªn link gá»‘c, chá»‰ thay náº¿u cáº§n
                                if "preset:view/plain" in img or "preset:listing" in img:
                                    images.append(img)
            except:
                pass
        # 2. Regex trong toÃ n bá»™ HTML/script - lá»c áº£nh sáº£n pháº©m tháº­t
        matches = re.findall(r'(https?://cdn\.chotot\.com/[^"\')\s<]+?\.(jpg|jpeg|png|webp))', resp.text)
        for m in matches:
            url = m[0]
            # Lá»c cháº·t: chá»‰ áº£nh cÃ³ ID dÃ i á»Ÿ cuá»‘i (áº£nh tin tháº­t), loáº¡i ads/logo/avatar
            if (re.search(r'-\d{15,}\.(jpg|jpeg|png|webp)$', url) and 
                "avatar" not in url and "logo" not in url and "admincentre" not in url and 
                "reward" not in url):
                if url not in images:
                    images.append(url)
        # 3. TÃ¬m video (náº¿u cÃ³ <video> hoáº·c thumbnail video)
        video_tags = soup.find_all("video")
        for vid in video_tags:
            src = vid.get("src") or ""
            if src and "cdn.chotot.com" in src:
                videos.append(src)
            sources = vid.find_all("source")
            for s in sources:
                src = s.get("src") or ""
                if src:
                    videos.append(src)
        # Náº¿u cÃ³ thumbnail video, láº¥y src thumbnail nhÆ°ng Ä‘Ã¡nh dáº¥u lÃ  video
        thumb_videos = soup.find_all("img", src=re.compile(r'videodelivery\.net.*thumbnail'))
        for thumb in thumb_videos:
            src = thumb.get("src") or ""
            if src:
                # Thá»­ thay thumbnail.jpg â†’ video.mp4 (cáº§n test vá»›i link tháº­t)
                videos.append(src.replace("thumbnail.jpg", "video.mp4"))
        # Giá»›i háº¡n & unique
        images = list(dict.fromkeys(images))[:6] # loáº¡i duplicate, giá»¯ 6
        videos = list(dict.fromkeys(videos))[:2]
        log(f"Detail {link}: {len(images)} áº£nh tháº­t + {len(videos)} video")
        return images, videos
    except Exception as e:
        log(f"Lá»—i láº¥y media {link}: {e}")
        return [], []
def send_telegram_with_media(item, images, videos):
    cfg = get_telegram_config()
    if not cfg["token"] or not cfg["chat_id"]:
        return
    caption = (
        f"ğŸ¸ <b>HÃ€NG Má»šI - CHá»¢ Tá»T</b>\n\n"
        f"<b>{item['title']}</b>\n"
        f"ğŸ’° <b>{item['price']}</b>\n"
        f"ğŸ‘¤ {item['seller']}\n"
        f"ğŸ‘€ {item['views']} views\n"
        f"ğŸ“ {item['location']}\n"
        f"â° {item['time']}\n\n"
        f"<a href='{item['link']}'>ğŸ”— Xem chi tiáº¿t</a>"
    )
    media_group = []
    # áº¢nh trÆ°á»›c
    for idx, img_url in enumerate(images):
        media_group.append({
            "type": "photo",
            "media": img_url,
            "caption": caption if idx == 0 and not videos else "", # caption á»Ÿ item Ä‘áº§u náº¿u cÃ³ áº£nh
            "parse_mode": "HTML"
        })
    # Video sau (Telegram há»— trá»£ mix trong sendMediaGroup)
    for vid_url in videos:
        media_group.append({
            "type": "video",
            "media": vid_url,
            "caption": caption if not media_group else "" # náº¿u khÃ´ng cÃ³ áº£nh
        })
    if media_group:
        url = f"https://api.telegram.org/bot{cfg['token']}/sendMediaGroup"
        payload = {"chat_id": cfg["chat_id"], "media": json.dumps(media_group)}
        try:
            resp = requests.post(url, data=payload, timeout=25)
            if resp.status_code == 200:
                log(f"âœ… Gá»­i media group ({len(images)} áº£nh + {len(videos)} video) cho tin má»›i")
            else:
                log(f"Telegram media group lá»—i {resp.status_code}: {resp.text[:200]}")
                # Náº¿u lá»—i (vÃ­ dá»¥ video khÃ´ng há»— trá»£), gá»­i text + áº£nh riÃªng
                send_telegram_alert(item)
        except Exception as e:
            log(f"Lá»—i gá»­i media: {e}")
            send_telegram_alert(item)
    else:
        send_telegram_alert(item)
def send_telegram_alert(item):
    cfg = get_telegram_config()
    if not cfg["token"] or not cfg["chat_id"]:
        return
    message = (
        f"ğŸ¸ <b>HÃ€NG Má»šI - CHá»¢ Tá»T</b>\n\n"
        f"<b>{item['title']}</b>\n"
        f"ğŸ’° <b>{item['price']}</b>\n"
        f"ğŸ‘¤ {item['seller']}\n"
        f"ğŸ‘€ {item['views']} views\n"
        f"ğŸ“ {item['location']}\n"
        f"â° {item['time']}\n\n"
        f"<a href='{item['link']}'>ğŸ”— Xem chi tiáº¿t</a>"
    )
    requests.post(
        f"https://api.telegram.org/bot{cfg['token']}/sendMessage",
        json={"chat_id": cfg["chat_id"], "text": message, "parse_mode": "HTML"},
        timeout=10
    )
def page_has_no_results(driver):
    try:
        text = driver.find_element(By.TAG_NAME, "body").text.lower()
        return any(x in text for x in ["khÃ´ng cÃ³ káº¿t quáº£", "khÃ´ng tÃ¬m tháº¥y", "0 tin Ä‘Äƒng"])
    except:
        return False
def extract_item_data(item_element, page_num):
    try:
        a = item_element.find_element(By.TAG_NAME, "a")
        link = a.get_attribute("href")
        if not link.startswith("http"):
            link = BASE_URL + link.strip()
        title = item_element.find_element(By.CSS_SELECTOR, "h3").text.strip() or "KhÃ´ng cÃ³ tiÃªu Ä‘á»"
        price = "Thá»a thuáº­n"
        try: price = item_element.find_element(By.CSS_SELECTOR, "span.bfe6oav").text.strip()
        except: pass
        time_posted = "N/A"
        try: time_posted = item_element.find_element(By.CSS_SELECTOR, "span.c1u6gyxh.tx5yyjc").text.strip()
        except: pass
        location = "HÃ  Ná»™i"
        try: location = item_element.find_element(By.CSS_SELECTOR, "span.c1u6gyxh:not(.tx5yyjc)").text.strip()
        except: pass
        seller = "áº¨n danh"
        try: seller = item_element.find_element(By.CSS_SELECTOR, "div.dteznpi span.brnpcl3").text.strip()
        except: pass
        views_str = "0"
        try: views_str = item_element.find_element(By.CSS_SELECTOR, "div.vglk6qt span").text.strip()
        except: pass
        views = int(''.join(c for c in views_str if c.isdigit())) if ''.join(c for c in views_str if c.isdigit()) else 0
        return {
            "title": title,
            "price": price,
            "link": link,
            "time": time_posted,
            "location": location,
            "seller": seller,
            "views": views,
            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "page": page_num
        }
    except:
        return None
def scrape_data():
    log("ğŸš€ Báº®T Äáº¦U QUÃ‰T CHá»¢ Tá»T - Nháº¡c cá»¥ HÃ  Ná»™i â‰¤ 2.1tr")
    worksheet = connect_google_sheet()
    # Map link â†’ row
    try:
        link_col = worksheet.col_values(3)
        link_to_row = {link.strip(): idx + 1 for idx, link in enumerate(link_col[1:], 1) if link.strip()}
        existing_links = set(link_to_row.keys())
        log(f"Äá»c {len(existing_links)} tin cÅ© tá»« sheet")
    except Exception as e:
        log(f"Lá»—i Ä‘á»c sheet: {e}")
        link_to_row = {}
        existing_links = set()
    driver = setup_driver()
    total_new = 0
    total_updated = 0
    page = 1
    consecutive_empty = 0
    while page <= MAX_PAGES:
        url = START_URL if page == 1 else f"{START_URL}&page={page}"
        log(f" Trang {page} â†’ {url}")
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "li.a14axl8t")))
        except Exception as e:
            log(f"Load trang {page} lá»—i: {e}")
            if page_has_no_results(driver):
                log("Háº¿t káº¿t quáº£ â†’ dá»«ng")
                break
            consecutive_empty += 1
            if consecutive_empty >= MAX_CONSECUTIVE_EMPTY:
                log("Dá»«ng do nhiá»u trang rá»—ng")
                break
            page += 1
            time.sleep(SLEEP_BETWEEN_PAGES)
            continue
        if page_has_no_results(driver):
            break
        items = driver.find_elements(By.CSS_SELECTOR, "li.a14axl8t")
        log(f"TÃ¬m tháº¥y {len(items)} item")
        new_rows = []
        batch_requests = []
        for item_el in items:
            data = extract_item_data(item_el, page)
            if not data:
                continue
            link = data["link"]
            if link in link_to_row:
                row = link_to_row[link]
                batch_requests.append({
                    "range": f"G{row}", # Views - cá»™t G
                    "values": [[str(data["views"])]]
                })
                batch_requests.append({
                    "range": f"I{row}", # Hidden - cá»™t I
                    "values": [[str(page)]]
                })
                total_updated += 1
            else:
                existing_links.add(link)
                new_rows.append([
                    data["title"], data["price"], link, data["time"], data["location"],
                    data["seller"], str(data["views"]), data["scraped_at"], str(page)
                ])
                images, videos = get_images_from_detail(link)
                send_telegram_with_media(data, images, videos)
                total_new += 1
        if new_rows:
            log(f"ThÃªm {len(new_rows)} tin má»›i tá»« trang {page}")
            # Append nhÆ° cÅ©
            worksheet.append_rows(new_rows)
            # Sau khi append â†’ sort toÃ n bá»™ sheet theo cá»™t Hidden (page) tÄƒng dáº§n
            try:
                # Láº¥y toÃ n bá»™ dá»¯ liá»‡u (bá» header)
                all_data = worksheet.get_all_values()[1:] # tá»« dÃ²ng 2 trá»Ÿ Ä‘i
                if all_data:
                    # Sort theo cá»™t 9 (Hidden, index 8), tÄƒng dáº§n (page nhá» lÃªn Ä‘áº§u)
                    sorted_data = sorted(
                        all_data,
                        key=lambda row: int(row[8]) if len(row) > 8 and row[8].isdigit() else 999, # cá»™t Hidden
                        reverse=False # tÄƒng dáº§n
                    )
                    # XÃ³a dá»¯ liá»‡u cÅ© (giá»¯ header)
                    worksheet.clear()
                    # Viáº¿t láº¡i header
                    worksheet.append_row(["Title", "Price", "Link", "Time Posted", "Location", "Seller", "Views", "Scraped At", "Hidden"])
                    # Viáº¿t láº¡i dá»¯ liá»‡u Ä‘Ã£ sort
                    worksheet.append_rows(sorted_data)
                    log(f"ÄÃ£ sort sheet theo Hidden (page) tÄƒng dáº§n ({len(sorted_data)} dÃ²ng)")
            except Exception as e:
                log(f"Lá»—i khi sort sheet: {e}")
        # Batch update views + hidden váº«n giá»¯ nguyÃªn (khÃ´ng áº£nh hÆ°á»Ÿng thá»© tá»±)
        if batch_requests:
            worksheet.batch_update(batch_requests)
            log(f"Batch update {len(batch_requests)//2} tin cÅ© trang {page}")
        if not new_rows and not batch_requests:
            consecutive_empty += 1
        else:
            consecutive_empty = 0
        page += 1
        time.sleep(SLEEP_BETWEEN_PAGES)
    driver.quit()
    log(f"HoÃ n thÃ nh: +{total_new} má»›i | â†‘{total_updated} cáº­p nháº­t")
if __name__ == "__main__":
    try:
        scrape_data()
    except Exception as e:
        log(f"Lá»—i chÃ­nh: {e}")
